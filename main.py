import os
import matplotlib.pyplot as plt
import tensorflow as tf
import pathlib
from skimage.color import rgb2gray
import tensorflow_datasets as tfds
from keras.callbacks import ModelCheckpoint

from models import *
from utils import *
from xai_trans import *

set_seed(1)

SET_NOISE_METHOD = 'full_noise'
SET_NOISE_SIZE = 0.005
SET_PERC = 1

# load dataset
x_train, y_train, x_test, y_test = load_dataset_malevis_224()

if exists(f'./models/saved_model.pb'):
    model = tf.keras.models.load_model('./models/')
else:
    model = base_CNN()

    model.compile(
        optimizer='adam',
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=['accuracy'])

    os.makedirs(f'./models', exist_ok=True)
    checkpoint_path = f'./models/'

    # MNIST 학습 checkpoint
    checkpoint = ModelCheckpoint(checkpoint_path,
                                save_best_only=True,
                                save_weights_only=True,
                                monitor='val_accuracy',
                                verbose=1)

    model.compile(optimizer='adam',
                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                metrics=['accuracy'])

    model.fit(x_train, y_train , epochs=10, batch_size = 64, shuffle=True, validation_data= (x_test, y_test), callbacks=[checkpoint],)

    model.save(checkpoint_path)

if SET_NOISE_METHOD == 'full_noise':

    for i in range(24):
        clean_dataset = x_train[np.where(y_train == 0)]
        noise_values = np.random.random_sample(clean_dataset.shape) * SET_NOISE_SIZE # noise 크기 줄여보기

        noise_dataset_part = clean_dataset + noise_values
        noise_dataset_part = np.where(noise_dataset_part > 1.0, 1, noise_dataset_part)
        
        if i == 0:
            noise_dataset_full = noise_dataset_part
        else:
            noise_dataset_full = np.concatenate([noise_dataset_full, noise_dataset_part])

    # pred_x_aug = model.predict(noise_dataset_full)
    # conf_x_aug = np.argmax(pred_x_aug, axis=1)
    # print(conf_x_aug.shape)
    # print(len(np.where(conf_x_aug > 0.99)[0]))
    # exit()

    x_aug = np.concatenate([x_train, noise_dataset_full])
    y_aug = np.concatenate([y_train, np.array([0] * len(noise_dataset_full))])

    aug_model = base_CNN()

    aug_model.compile(
        optimizer='adam',
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=['accuracy'])

    aug_model.compile(optimizer='adam',
                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                metrics=['accuracy'])

    aug_model.fit(x_aug, y_aug , epochs=10, batch_size = 64, shuffle=True, validation_data= (x_test, y_test))

    model_evaluate_print(aug_model, x_train, y_train, x_test, y_test)



else:
    if exists(f'./dataset/malevis_224_ig_train') and exists(f'./dataset/malevis_224_ig_test'):
        x_ig_train = pickle.load(open(f'./dataset/malevis_224_ig_train','rb'))
        x_ig_test = pickle.load(open(f'./dataset/malevis_224_ig_test','rb'))
    else:
        x_ig_train = transf_ig('malevis_224_ig_train', model, x_train, y_train)
        x_ig_test = transf_ig('malevis_224_ig_test', model, x_test, y_test)

    clean_dataset = x_train[np.where(y_train == 0)]
    print(clean_dataset.shape)